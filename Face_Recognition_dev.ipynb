{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4b1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def imshow(image):\n",
    "    image = cv2.resize(image, (720, 480))\n",
    "    cv2.imshow('img', image)\n",
    "    while(True):\n",
    "        k = cv2.waitKey(33)\n",
    "        if k == -1:  # if no key was pressed, -1 is returned\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    cv2.destroyWindow('img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4b4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'D:\\Face Emotion\\VNIT_CSE_project data\\Work2_final_implementation_08.10.2021_MTCNN\\models\\FaceNet\\model-20211123T073420Z-001\\model\\facenet_keras.h5'\n",
    "img_path = r'C:\\Users\\vb359\\Desktop\\classroom.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988228bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 26 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E928225AF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# function for face detection with mtcnn\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    # load image from file\n",
    "    image = Image.open(filename)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = asarray(image)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "\n",
    "# load the photo and extract the face\n",
    "pixels = extract_face(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98977ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = np.expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d08e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
      "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
     ]
    }
   ],
   "source": [
    "# example of loading the keras facenet model\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "model = load_model(model_path)\n",
    "# summarize input and output shape\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e430fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1766326 ,  1.9056361 , -0.7613096 , -1.3866025 , -2.2703023 ,\n",
       "        1.1852049 , -0.20646556, -1.0285273 ,  0.18224674, -1.2138195 ,\n",
       "       -0.3897837 ,  0.7750747 ,  0.40079635, -0.6088139 , -0.11867131,\n",
       "        0.967562  , -0.204104  , -0.57183474,  1.125838  , -1.0514364 ,\n",
       "       -0.26185855,  0.03083398,  0.47595835,  0.11156639,  0.5432742 ,\n",
       "       -0.53217614, -0.85850656, -0.25983486, -0.6262243 ,  1.8947592 ,\n",
       "       -0.12196824, -2.0747833 , -0.01736911,  0.7592115 ,  0.76181114,\n",
       "       -1.7310779 , -0.41452488,  2.3053243 ,  1.2741888 ,  2.8080034 ,\n",
       "       -0.8641908 ,  0.04294378, -1.678656  , -0.50730765, -0.208897  ,\n",
       "       -0.2825104 ,  0.6216603 ,  1.1852691 ,  0.96457213, -0.3456197 ,\n",
       "       -1.2692329 ,  1.153424  ,  1.0181694 , -1.2982844 ,  1.7547283 ,\n",
       "        0.7074365 ,  1.5245467 ,  0.40899193,  0.7098911 ,  0.92599547,\n",
       "        1.8356247 , -0.05091607, -0.438646  , -0.84878457,  0.1621604 ,\n",
       "       -0.29447806, -0.7768699 ,  0.69195235, -1.8609977 , -1.6755624 ,\n",
       "        0.27192697,  0.5722704 ,  0.7127549 , -0.8800494 ,  0.7422325 ,\n",
       "        1.2394272 , -0.97489834,  1.5512173 ,  0.43298495, -1.8760345 ,\n",
       "       -0.39634904, -0.5833529 , -1.0054885 ,  0.40361995,  1.3885018 ,\n",
       "        0.23772705, -0.84317434, -0.24916144, -0.6817179 , -0.07187657,\n",
       "        0.97583824, -0.14734511,  1.9659734 , -0.7343801 ,  0.71773845,\n",
       "        1.6923931 , -0.6439364 ,  1.3731335 , -0.93009114, -1.2843432 ,\n",
       "        2.096478  , -0.25692824,  0.32237887, -0.21543257, -0.37723562,\n",
       "       -1.0896229 , -1.4670794 ,  1.8008804 , -0.6988588 , -1.6557543 ,\n",
       "        0.11269502, -0.32049465, -0.14986515, -0.68161297, -1.5873135 ,\n",
       "        0.08969708, -0.59225714,  0.66532856,  0.84164006,  2.4199042 ,\n",
       "        0.78493303,  0.19955501, -1.1254622 , -1.6623882 ,  0.23329116,\n",
       "       -0.00632753, -1.3053678 , -2.1080766 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(model, pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c2ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
